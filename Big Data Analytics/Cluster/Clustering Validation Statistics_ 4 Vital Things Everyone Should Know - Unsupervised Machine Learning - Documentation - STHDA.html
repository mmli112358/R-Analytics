<!DOCTYPE html>
<html lang="en">
	<head>
		<title>Clustering Validation Statistics: 4 Vital Things Everyone Should Know - Unsupervised Machine Learning - Documentation - STHDA</title>
		<meta charset="iso-8859-1" />
		
		<meta name="keywords" content="R, statistics, graph, data analysis, training courses in R genomics, sequencing, microarray, gene expression." />
		<meta name="generator" content="PHPBoost 4.0" />
		
		
		<!-- Theme CSS -->
		
		<link rel="stylesheet" href="/english/cache/css/css-cache-7921c200b2f09b704d0a1d0ad31fc770.css" type="text/css" media="screen, print, handheld" />
		
		
		<!-- Modules CSS -->
		<link rel="stylesheet" href="/english/cache/css/css-cache-e4be9317e570757e9bba0c4ca228015c.css" type="text/css" media="screen, print, handheld" />

		
		<link rel="shortcut icon" href="/english/logo_mini.png" type="image/png" />
		
		
				<script>
		<!--
			var PATH_TO_ROOT = "/english";
			var TOKEN = "74c136fb55c47d73";
			var THEME = "sthda";
			var LANG = "english";
		-->
		</script>
		<script src="/english/kernel/lib/js/top.js"></script>
        
         <!--inclusion de jquery à partir de google si internet et sinon chargement local -->
		<script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js" ></script>
		
        <script>window.jQuery || document.write('<script src="/english/sthda/js/jquery-1.11.1.min.js"><\/script>')</script>
        <script>jQuery.noConflict();  // Use jQuery via jQuery(...)</script>
        <!-- Cookies reglementation européenne -->
        <!-- Begin Cookie Consent plugin by Silktide - http://silktide.com/cookieconsent -->
		<script type="text/javascript">
		    window.cookieconsent_options = {"message":"This website uses cookies to ensure you get the best experience on our website.","dismiss":"OK!","learnMore":"More info","link":"https://www.google.com/policies/technologies/cookies/","theme":"light-top"};
		</script>
		<script type="text/javascript" src="//s3.amazonaws.com/cc.silktide.com/cookieconsent.latest.min.js"></script>
		<!-- End Cookie Consent plugin -->

	</head>

	<body itemscope="itemscope" itemtype="http://schema.org/WebPage">
			
	<header id="header">
		<div id="top-header">
			<div id="site-infos" >
				<div id="site-logo" style="background: url('/english/images/customization/all_logo_80.png') no-repeat;"></div>
				<div id="site-name-container">
					<a id="site-name" href="/english/">STHDA</a>
                    <span style="color:white; font-size:12px;">
                        <!-- Langue -->
                        <a href="http://www.sthda.com/french" style="color:white;"> 
                                <img src="/english/images/stats/countries/fr.png" class="valign_middle" style="width:15px;"/></a> 
                        <a href="http://www.sthda.com/english" style="color:white;">
                            <img src="/english/images/stats/countries/uk.png" class="valign_middle" style="width:15px;"/></a>
                    </span>
					<span id="site-slogan">Statistical tools for high-throughput data analysis</span>
				</div>
                
                
	<script>
	<!--
	function check_connect()
	{
		if( document.getElementById('login').value == "" )
		{
			alert("Please enter a nickname !");
			return false;
		}
		if( document.getElementById('password').value == "" )
		{
			alert("Please enter a password !");
			return false;
		   }
	}
	-->
	</script>


	
	<div id="connect-menu">
		<div class="horizontal-fieldset">
			<ul class="connect-content">
				<li><a href="https://www.facebook.com/1570814953153056" class="facebook" target="_blank">
                	<i class="fa fa-facebook"></i><span>Facebook</span></a></li>
				<!--<li><a href="https://twitter.com/"><i class="fa fa-twitter"></i><span>Twitter</span></a></li>-->
				<li><a href="https://plus.google.com/108962828449690000520" rel="publisher" class="google" target="_blank">
                	<i class="fa fa-google-plus"></i><span>Google+</span></a></li>
				<li><a href="/english/user/connect/" class="small"> <i class="fa fa-sign-in"></i> <span>Log in</span></a></li>
				
				<li><a href="/english/user/registration/" class="small"> <i class="fa fa-pencil"></i> <span>Sign up</span></a></li>
				
			</ul>
		</div>
	</div>
    
	

                
			</div>
			
		</div>
		<div id="sub-header">
            <div style="max-width: 940px; margin:auto;;">
            <div id="navigation-menu" >
                    <span><a href="/english/"><i class="fa fa-home"></i>&nbsp;HOME</a></span>
                    <span><a href="/english/download/category-7+ebooks.php"><i class="fa fa-folder-open"></i>&nbsp;BOOKS</a></span>
                    <span><a href="/english/wiki/r-software"><i class="fa fa-area-chart"></i>&nbsp;R/STATISTICS</a></span>
                    <span><a href="/english/rsthda"><i class="fa fa-cogs"></i>&nbsp;WEB APPLICATIONS</a></span>
                    <span><a href="/english/contact/"><i class="fa fa-envelope"></i>&nbsp;CONTACT</a></span>
            </div>
            
            
<!-- google search -->

   <div style="height:30px; margin-top:2px;">
       <form action="http://www.sthda.com/english/googlesearch/result.php" id="cse-search-box">
          <div>
            <input type="hidden" name="cx" value="partner-pub-5474463749888038:6267345768" />
            <input type="hidden" name="cof" value="FORID:10" />
            <input type="hidden" name="ie" value="UTF-8" />
            <input type="text" name="q" size="55" />
            <input type="submit" name="sa" value="Search" class="submitBtn" />
          </div>
        </form>
        
        <script type="text/javascript" src="http://www.google.com/coop/cse/brand?form=cse-search-box&amp;lang=en"></script>
    </div>
      
 <!-- End google search -->
 
 <style>
.submitBtn {
	height: auto;
	padding: 4px;
	color: #333333;
	text-align: center;
	text-shadow: 0 1px 1px rgba(255, 255, 255, 0.1);
	background-image: linear-gradient(to bottom,  rgba(255,255,255,0.18) 0%, rgba(56,56,56,0.10) 100%);
	background-color: #F9F9F9;
	border: 1px solid #CCCCCC;
	border-color: #E1E1E1 #E1E1E1 #BFBFBF #CFCFCF;
	border-radius: 4px;
	box-shadow: inset 0 0 0 rgba(255, 255, 255, 0.2), 0 0px 2px rgba(0, 0, 0, 0.05);
	color: #FEFEFE;
	background-color: #3B6B9F;
	border-color: #366393;
}
.submitBtn{cursor:pointer;}
</style>
            
                
            </div>
		</div>
		<div class="spacer"></div>
	</header>
	
	<div id="global">
		
		
		
		
		
		
		<div id="main" role="main">
			
			<div id="main-content" itemprop="mainContentOfPage">
            
            
            	<div style="width:100%;height:15px; background-color:white; margin-left:2px; margin-bottom:10px;">
                    <!-- Adsense  Link -->
                    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
                    <!-- liens_728X15 -->
                    <ins class="adsbygoogle"
                         style="display:inline-block;width:728px;height:15px"
                         data-ad-client="ca-pub-5474463749888038"
                         data-ad-slot="3453480168"></ins>
                    <script>
                    (adsbygoogle = window.adsbygoogle || []).push({});
                    </script>
                </div>
        
        
        
				
<menu id="actions-links-menu" class="dynamic-menu right">
	<ul>
		<li><a><i class="fa fa-cog"></i></a>
			<ul>
				
					<li ><a href="/english/wiki/wiki.php">Home</a>
	
</li>
				
					<li ><a href="/english/wiki/explorer.php">Explorer</a>
	
</li>
				
			</ul>
		</li>
	</ul>
</menu>

				<nav id="breadcrumb" itemprop="breadcrumb">
					<ol>
						<li itemscope itemtype="http://data-vocabulary.org/Breadcrumb">
							<a href="/english/" title="Home" itemprop="url">
								<span itemprop="title">Home</span>
							</a>
						</li>
						
							<li itemscope itemtype="http://data-vocabulary.org/Breadcrumb" >
								
								<a href="wiki.php" title="Documentation" itemprop="url">
									<span itemprop="title">Documentation</span>
								</a>
								
							</li>
						
							<li itemscope itemtype="http://data-vocabulary.org/Breadcrumb" >
								
								<a href="r-software" title="R software" itemprop="url">
									<span itemprop="title">R software</span>
								</a>
								
							</li>
						
							<li itemscope itemtype="http://data-vocabulary.org/Breadcrumb" >
								
								<a href="clustering-unsupervised-machine-learning" title="Clustering - Unsupervised machine learning" itemprop="url">
									<span itemprop="title">Clustering - Unsupervised machine learning</span>
								</a>
								
							</li>
						
							<li itemscope itemtype="http://data-vocabulary.org/Breadcrumb"  class="current" >
								
								<span itemprop="title">Clustering Validation Statistics: 4 Vital Things Everyone Should Know - Unsupervised Machine Learning</span>
								
							</li>
						
					</ol>
				</nav>
				
     <style>
	 /*link color*/
	  a{color:#0053F9;} .wiki a:hover{color:red!important;}
     </style>
       
       <div class="wiki">
       
        <article>
            
			<header>
				<h1>
					<a href="/english/syndication/rss/wiki/34" title="Syndication" class="fa fa-syndication"></a>
					Clustering Validation Statistics: 4 Vital Things Everyone Should Know - Unsupervised Machine Learning
				</h1>
			</header>
            
           
            
            
			<div class="content">
						<div style="margin-bottom:10px;">
			<menu class="dynamic-menu right group">
				<ul>
				
					<li>
						<a href="property.php?idcom=241&amp;com=0"><i class="fa fa-comments-o"></i> Discussion</a>
					</li>
				
					<li>
						<a><i class="fa fa-cog"></i> Tools</a>
						<ul>

							

							<!--
							AK: Inactivation historique/duplicated content
							<li><a href="../wiki/history.php?id=241" title="History">
								<i class="fa fa-reply"></i> History
							</a> 
							</li>
							-->
						

							
							
								
								
								
								
								
								
								
								
							
							
							
								<!--
								 AK print
								<li><a href="../wiki/print.php?id=241" title="Printable version">
									<i class="fa fa-print"></i> Printable version
								</a></li>
							   -->
							
						</ul>
					</li>
				</ul>
			</menu>
		</div>
		<div  class="spacer" style="margin-top:15px;">&nbsp;</div>
				
				
				
				
				
				
				
				
                
                <br/><br/>


                <div id ="sticky-parent">
                
                	 <!--side bar -->
                    <div  style="float:left; width:300px; min-height:700px; text-align:center;" >
                        <div id="aksidebar">

                        <!-- Adsense -->
                        <div>
                        	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
                            <!-- 300X600 -->
                            <ins class="adsbygoogle"
                                 style="display:inline-block;width:300px;height:600px"
                                 data-ad-client="ca-pub-5474463749888038"
                                 data-ad-slot="6825748964"></ins>
                            <script>
                            (adsbygoogle = window.adsbygoogle || []).push({});
                            </script>

                         </div>


                         <br/><br/>
                         <div>
                            <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
                            <!-- lien_200X90 -->
                            <ins class="adsbygoogle"
                                 style="display:inline-block;width:200px;height:90px"
                                 data-ad-client="ca-pub-5474463749888038"
                                 data-ad-slot="7994647366"></ins>
                            <script>
                            (adsbygoogle = window.adsbygoogle || []).push({});
                            </script>
                        </div>
                        <br/><br/>
                        <!-- /Adsense -->

                         <!-- Publicite AK -->
                        <div id ="pub"  style="width:300px; text-align:left; margin-top:15px;">
                            <div id="ebook">
                                <div style = "color:#A85F16; font-size:1.5em;"><i class="fa fa-book fa-3x"></i> Download R Books</div><br/>
                                    <a href="http://www.sthda.com/english/download/download-6+complete-guide-to-3d-plots-in-r.php" target ="_blank">
                                         <i class ="fa fa-book fa-2x"></i> Complete Guide to 3D Plots in R: Static and interactive 3-dimension graphs</a><br/>

                                    <a href="http://www.sthda.com/english/download/download-5+ggplot2-the-elements-for-elegant-data-visualization-in-r.php" target ="_blank"> <i class ="fa fa-book fa-2x"></i> ggplot2: The Elements for Elegant Data Visualization in R</a><br/>
                                
                                <!--
                                <a href="http://www.sthda.com/english/download/download-6+complete-guide-to-3d-plots-in-r.php" target ="_blank">
                                    <b>3D Plots in R</b> <br/><br/>
                                    <img src="http://www.sthda.com/sthda/RDoc/images/3d-graphic-cover.png"/>
                                </a>
                            -->
                            </div>
                        </div>
                        <br/><br/>
                        <!-- end pub ak-->


                         <div>
                            
                         </div>
                     </div>
                     <div class="sticky-content-spacer"></div>
                    </div>

                    
                
                	<!-- content -->
                    <div style="width:580px; float:right;" id = "ak_main">
                        


                    	<div>
                    		<!-- START HTML -->
  
  <!--====================== start from here when you copy to sthda================-->  
  <div id="rdoc">

<div id="TOC">
<ul>
<li><a href="#required-packages"><span class="toc-section-number">1</span> Required packages</a></li>
<li><a href="#data-preparation"><span class="toc-section-number">2</span> Data preparation</a></li>
<li><a href="#relative-measures-determine-the-optimal-number-of-clusters"><span class="toc-section-number">3</span> Relative measures: Determine the optimal number of clusters</a></li>
<li><a href="#clustering-analysis"><span class="toc-section-number">4</span> Clustering analysis</a><ul>
<li><a href="#example-of-partitioning-method-results"><span class="toc-section-number">4.1</span> Example of partitioning method results</a></li>
<li><a href="#example-of-hierarchical-clustering-results"><span class="toc-section-number">4.2</span> Example of hierarchical clustering results</a></li>
</ul></li>
<li><a href="#internal-clustering-validation-measures"><span class="toc-section-number">5</span> Internal clustering validation measures</a><ul>
<li><a href="#silhouette-analysis"><span class="toc-section-number">5.1</span> Silhouette analysis</a><ul>
<li><a href="#concept-and-algorithm"><span class="toc-section-number">5.1.1</span> Concept and algorithm</a></li>
<li><a href="#interpretation-of-silhouette-width"><span class="toc-section-number">5.1.2</span> Interpretation of silhouette width</a></li>
<li><a href="#r-functions-for-silhouette-analysis"><span class="toc-section-number">5.1.3</span> R functions for silhouette analysis</a></li>
<li><a href="#silhouette-plot-for-k-means-clustering"><span class="toc-section-number">5.1.4</span> Silhouette plot for k-means clustering</a></li>
<li><a href="#silhouette-plot-for-pam-clustering"><span class="toc-section-number">5.1.5</span> Silhouette plot for PAM clustering</a></li>
<li><a href="#silhouette-plot-for-hierarchical-clustering"><span class="toc-section-number">5.1.6</span> Silhouette plot for hierarchical clustering</a></li>
<li><a href="#samples-with-a-negative-silhouette-coefficient"><span class="toc-section-number">5.1.7</span> Samples with a negative silhouette coefficient</a></li>
</ul></li>
<li><a href="#dunn-index"><span class="toc-section-number">5.2</span> Dunn index</a><ul>
<li><a href="#concept-and-algorithm-1"><span class="toc-section-number">5.2.1</span> Concept and algorithm</a></li>
<li><a href="#r-function-for-computing-dunn-index"><span class="toc-section-number">5.2.2</span> R function for computing Dunn index</a></li>
</ul></li>
<li><a href="#clustering-validation-statistics"><span class="toc-section-number">5.3</span> Clustering validation statistics</a></li>
</ul></li>
<li><a href="#external-clustering-validation"><span class="toc-section-number">6</span> External clustering validation</a></li>
<li><a href="#infos"><span class="toc-section-number">7</span> Infos</a></li>
</ul>
</div>

<p><br/></p>
<p><strong>Clustering</strong> is an <strong>unsupervised machine learning</strong> method for partitioning dataset into a set of groups or clusters. A big issue is that <strong>clustering methods</strong> will return clusters even if the data does not contain any clusters. Therefore, its necessary i) to <a href="http://www.sthda.com/english/wiki/assessing-clustering-tendency-a-vital-issue-unsupervised-machine-learning"><strong>assess clustering tendency</strong></a> before the analysis and ii) to validate the quality of the result after clustering.</p>
<p>A variety of measures has been proposed in the literature for evaluating clustering results. The term <strong>clustering validation</strong> is used to design the procedure of evaluating the results of a clustering algorithm.</p>
<p>Generally, clustering validation statistics can be categorized into 4 classes (Theodoridis and Koutroubas, 2008; G. Brock et al., 2008, Charrad et al., 2014):</p>
<br/>
<div class="block">
<ol style="list-style-type: decimal">
<li><p><strong>Relative clustering validation</strong>, which evaluates the clustering structure by varying different parameter values for the same algorithm (e.g.,: varying the number of clusters k). Its generally used for determining the <a href="http://www.sthda.com/english/wiki/determining-the-optimal-number-of-clusters-3-must-known-methods-unsupervised-machine-learning">optimal number of clusters</a>.</p></li>
<li><p><strong>External clustering validation</strong>, which consists in comparing the results of a cluster analysis to an externally known result, such as externally provided class labels. Since we know the true cluster number in advance, this approach is mainly used for selecting the right clustering algorithm for a specific dataset.</p></li>
<li><p><strong>Internal clustering validation</strong>, which use the internal information of the clustering process to evaluate the goodness of a clustering structure without reference to external information. It can be also used for estimating the number of clusters and the appropriate clustering algorithm without any external data.</p></li>
<li><strong>Clustering stability validation</strong>, which is a special version of internal validation. It evaluates the consistency of a clustering result by comparing it with the clusters obtained after each column is removed, one at a time. <strong>Clustering stability measures</strong> will be described in a future chapter.</li>
</ol>
</div>
<p><br/></p>
<p>The aim of this article is to:</p>
<ul>
<li>describe the different methods for clustering validation</li>
<li>compare the quality of clustering results obtained with different clustering algorithms</li>
<li>provide R lab section for validating clustering results</li>
</ul>
<p><span class="notice"> In all the examples presented here, well apply <strong>k-means</strong>, <strong>PAM</strong> and <strong>hierarchical</strong> clustering. Note that, the functions used in this article can be applied to evaluate the validity of any other clustering methods.</span></p>
<div id="required-packages" class="section level1">
<h1><span class="header-section-number">1</span> Required packages</h1>
<p>The following packages will be used:</p>
<ul>
<li><strong>cluster</strong> for computing <strong>PAM clustering</strong> and for analyzing <strong>cluster silhouettes</strong></li>
<li><strong>factoextra</strong> for simplifying clustering workflows and for visualizing clusters using <strong>ggplot2</strong> plotting system</li>
<li><strong>NbClust</strong> for determining the optimal number of clusters in the data</li>
<li><strong>fpc</strong> for computing clustering validation statistics</li>
</ul>
<p>Install <strong>factoextra</strong> package as follow:</p>
<pre class="r"><code>if(!require(devtools)) install.packages(&quot;devtools&quot;)
devtools::install_github(&quot;kassambara/factoextra&quot;)</code></pre>
<p>The remaining packages can be installed using the code below:</p>
<pre class="r"><code>pkgs &lt;- c(&quot;cluster&quot;, &quot;fpc&quot;, &quot;NbClust&quot;)
install.packages(pkgs)</code></pre>
<p>Load packages:</p>
<pre class="r"><code>library(factoextra)
library(cluster)
library(fpc)
library(NbClust)</code></pre>
</div>
<div id="data-preparation" class="section level1">
<h1><span class="header-section-number">2</span> Data preparation</h1>
<p>The data set <em>iris</em> is used. We start by excluding the column Species and scaling the data using the function <strong>scale()</strong>:</p>
<pre class="r"><code># Load the data
data(iris)
head(iris)</code></pre>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa</code></pre>
<pre class="r"><code># Remove species column (5) and scale the data
iris.scaled &lt;- scale(iris[, -5])</code></pre>
<p><span class="notice">Iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.</span></p>
</div>
<div id="relative-measures-determine-the-optimal-number-of-clusters" class="section level1">
<h1><span class="header-section-number">3</span> Relative measures: Determine the optimal number of clusters</h1>
<p>Many indices (more than 30) has been published in the literature for finding the right number of clusters in a dataset. The process has been covered in my previous article: <a href="http://www.sthda.com/english/wiki/determining-the-optimal-number-of-clusters-3-must-known-methods-unsupervised-machine-learning">Determining the optimal number of clusters</a>.</p>
<p>In this section well use the package <strong>NbClust</strong> which will compute, with a single function call, 30 indices for deciding the right number of clusters in the dataset:</p>
<pre class="r"><code># Compute the number of clusters
library(NbClust)
nb &lt;- NbClust(iris.scaled, distance = &quot;euclidean&quot;, min.nc = 2,
        max.nc = 10, method = &quot;complete&quot;, index =&quot;all&quot;)</code></pre>
<pre class="r"><code># Visualize the result
library(factoextra)
fviz_nbclust(nb) + theme_minimal()</code></pre>
<pre><code>## Among all indices: 
## ===================
## * 2 proposed  0 as the best number of clusters
## * 1 proposed  1 as the best number of clusters
## * 2 proposed  2 as the best number of clusters
## * 18 proposed  3 as the best number of clusters
## * 3 proposed  10 as the best number of clusters
## 
## Conclusion
## =========================
## * Accoridng to the majority rule, the best number of clusters is  3 .</code></pre>
<p><img src="http://www.sthda.com/sthda/RDoc/figure/clustering/clustering-validation-statistics-nbclust-factoextra-1.png" title="Clustering validation statistics - Unsupervised Machine Learning" alt="Clustering validation statistics - Unsupervised Machine Learning" width="518.4" /></p>
</div>
<div id="clustering-analysis" class="section level1">
<h1><span class="header-section-number">4</span> Clustering analysis</h1>
<p>Well use the function <strong>eclust()</strong> [in <strong>factoextra</strong>] which provides several advantages as described in the previous chapter: <a href="http://www.sthda.com/english/wiki/visual-enhancement-of-clustering-analysis-unsupervised-machine-learning">Visual Enhancement of Clustering Analysis</a>.</p>
<p><strong>eclust()</strong> stands for enhanced clustering. It simplifies the workflow of clustering analysis and, it can be used to compute <a href="http://www.sthda.com/english/wiki/hierarchical-clustering-essentials-unsupervised-machine-learning">hierarchical clustering</a> and <a href="http://www.sthda.com/english/wiki/partitioning-cluster-analysis-quick-start-guide-unsupervised-machine-learning">partititioning clustering</a> in a single line function call.</p>
<div id="example-of-partitioning-method-results" class="section level2">
<h2><span class="header-section-number">4.1</span> Example of partitioning method results</h2>
<p><strong>K-means</strong> and <strong>PAM</strong> clustering are described in this section. Well split the data into 3 clusters as follow:</p>
<pre class="r"><code># K-means clustering
km.res &lt;- eclust(iris.scaled, &quot;kmeans&quot;, k = 3,
                 nstart = 25, graph = FALSE)
# k-means group number of each observation
km.res$cluster</code></pre>
<pre><code>##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 3 3 2 3 3 3 3 3 3 3 3 2 3 3 3 3
##  [71] 2 3 3 3 3 2 2 2 3 3 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 2 2 2
## [106] 2 3 2 2 2 2 2 2 3 3 2 2 2 2 3 2 3 2 3 2 2 3 2 2 2 2 2 2 3 3 2 2 2 3 2
## [141] 2 2 3 2 2 2 3 2 2 3</code></pre>
<pre class="r"><code># Visualize k-means clusters
fviz_cluster(km.res, geom = &quot;point&quot;, frame.type = &quot;norm&quot;)</code></pre>
<p><img src="http://www.sthda.com/sthda/RDoc/figure/clustering/clustering-validation-statistics-k-means-pam-clusterings-visualization-1.png" title="Clustering validation statistics - Unsupervised Machine Learning" alt="Clustering validation statistics - Unsupervised Machine Learning" width="518.4" /></p>
<pre class="r"><code># PAM clustering
pam.res &lt;- eclust(iris.scaled, &quot;pam&quot;, k = 3, graph = FALSE)
pam.res$cluster</code></pre>
<pre><code>##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 3 3 2 3 3 3 3 3 3 3 3 2 3 3 3 3
##  [71] 3 3 3 3 3 2 2 2 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 2 2 2
## [106] 2 3 2 2 2 2 2 2 3 2 2 2 2 2 3 2 3 2 3 2 2 3 3 2 2 2 2 2 3 3 2 2 2 3 2
## [141] 2 2 3 2 2 2 3 2 2 3</code></pre>
<pre class="r"><code># Visualize pam clusters
fviz_cluster(pam.res, geom = &quot;point&quot;, frame.type = &quot;norm&quot;)</code></pre>
<p><img src="http://www.sthda.com/sthda/RDoc/figure/clustering/clustering-validation-statistics-k-means-pam-clusterings-visualization-2.png" title="Clustering validation statistics - Unsupervised Machine Learning" alt="Clustering validation statistics - Unsupervised Machine Learning" width="518.4" /></p>
<p>Read more about partitioning methods: <a href="http://www.sthda.com/english/wiki/partitioning-cluster-analysis-quick-start-guide-unsupervised-machine-learning">Partitioning clustering</a></p>
</div>
<div id="example-of-hierarchical-clustering-results" class="section level2">
<h2><span class="header-section-number">4.2</span> Example of hierarchical clustering results</h2>
<pre class="r"><code># Enhanced hierarchical clustering
res.hc &lt;- eclust(iris.scaled, &quot;hclust&quot;, k = 3,
                method = &quot;complete&quot;, graph = FALSE) 
head(res.hc$cluster, 15)</code></pre>
<pre><code>##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 
##  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1</code></pre>
<pre class="r"><code># Dendrogram
fviz_dend(res.hc, rect = TRUE, show_labels = FALSE) </code></pre>
<p><img src="http://www.sthda.com/sthda/RDoc/figure/clustering/clustering-validation-statistics-unnamed-chunk-5-1.png" title="Clustering validation statistics - Unsupervised Machine Learning" alt="Clustering validation statistics - Unsupervised Machine Learning" width="518.4" /></p>
<p>Read more about hierarchical clustering: <a href="http://www.sthda.com/english/wiki/hierarchical-clustering-essentials-unsupervised-machine-learning">Hierarchical clustering</a></p>
</div>
</div>
<div id="internal-clustering-validation-measures" class="section level1">
<h1><span class="header-section-number">5</span> Internal clustering validation measures</h1>
<p>In this section, we describe the most widely used clustering validation indices. Recall that the goal of <strong>clustering algorithms</strong> is to split the dataset into clusters of objects, such that:</p>
<ul>
<li>the objects in the same cluster are similar as much as possible,</li>
<li>and the objects in different clusters are highly distinct</li>
</ul>
<p><span class="success"> That is, we want the <strong>average distance within cluster</strong> to be as small as possible; and the <strong>average distance between clusters</strong> to be as large as possible.</span></p>
<p>Internal validation measures reflect often the <strong>compactness</strong>, the <strong>connectedness</strong> and <strong>separation</strong> of the cluster partitions.</p>
<br/>
<div class="block">
<ol style="list-style-type: decimal">
<li><p><strong>Compactness measures</strong> evaluate how close are the objects within the same cluster. A lower <strong>within-cluster variation</strong> is an indicator of a good compactness (i.e., a good clustering). The different indices for evaluating the compactness of clusters are base on distance measures such as the cluster-wise within average/median distances between observations.</p></li>
<li><strong>Separation measures</strong> determine how well-separated a cluster is from other clusters. The indices used as separation measures include:
<ul>
<li>distances between cluster centers</li>
<li>the pairwise minimum distances between objects in different clusters</li>
</ul></li>
<li><strong>Connectivity</strong> corresponds to what extent items are placed in the same cluster as their nearest neighbors in the data space. The connectivity has a value between 0 and infinity and should be minimized.</li>
</ol>
</div>
<p><br/></p>
<p>Generally most of the indices used for internal clustering validation combine compactness and separation measures as follow:</p>
<p><span class="math">\[
Index = \frac{(\alpha \times Separation)}{(\beta \times Compactness)}
\]</span></p>
<p>Where <span class="math">\(\alpha\)</span> and <span class="math">\(\beta\)</span> are weights.</p>
<p><span class="success"> In this section, well describe the two commonly used indices for assessing the goodness of clustering: <strong>silhouette width</strong> and <strong>Dunn index</strong>.</span></p>
<p><span class="notice">Recall that, more than 30 indices has been published in literature. They can be easily computed using the function <strong>NbClust</strong> which has been described in my previous article: <a href="http://www.sthda.com/english/wiki/determining-the-optimal-number-of-clusters-3-must-known-methods-unsupervised-machine-learning">Determining the optimal number of clusters</a>.</span></p>
<div id="silhouette-analysis" class="section level2">
<h2><span class="header-section-number">5.1</span> Silhouette analysis</h2>
<div id="concept-and-algorithm" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Concept and algorithm</h3>
<p><strong>Silhouette analysis</strong> measures how well an observation is clustered and it estimates the <strong>average distance between clusters</strong>. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters.</p>
<p>For each observation <span class="math">\(i\)</span>, the silhouette width <span class="math">\(s_i\)</span> is calculated as follows:</p>
<br/>
<div class="block">
<ol style="list-style-type: decimal">
<li>For each observation <span class="math">\(i\)</span>, calculate the average dissimilarity <span class="math">\(a_i\)</span> between <span class="math">\(i\)</span> and all other points of the cluster to which i belongs.<br /></li>
<li><p>For all other clusters <span class="math">\(C\)</span>, to which i does not belong, calculate the average dissimilarity <span class="math">\(d(i, C)\)</span> of <span class="math">\(i\)</span> to all observations of C. The smallest of these <span class="math">\(d(i,C)\)</span> is defined as <span class="math">\(b_i= \min_C d(i,C)\)</span>. The value of <span class="math">\(b_i\)</span> can be seen as the dissimilarity between <span class="math">\(i\)</span> and its neighbor cluster, i.e., the nearest one to which it does not belong.</p></li>
<li>Finally the <strong>silhouette width</strong> of the observation <span class="math">\(i\)</span> is defined by the formula: <span class="math">\(S_i = (b_i - a_i)/max(a_i, b_i)\)</span>.<br /></li>
</ol>
</div>
<p><br/></p>
</div>
<div id="interpretation-of-silhouette-width" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Interpretation of silhouette width</h3>
<p>Silhouette width can be interpreted as follow:</p>
<br/>
<div class="block">
<ul>
<li><p>Observations with a large <span class="math">\(S_i\)</span> (almost 1) are very well clustered</p></li>
<li><p>A small <span class="math">\(S_i\)</span> (around 0) means that the observation lies between two clusters</p></li>
<li>Observations with a negative <span class="math">\(S_i\)</span> are probably placed in the wrong cluster.<br /></li>
</ul>
</div>
<p><br/></p>
</div>
<div id="r-functions-for-silhouette-analysis" class="section level3">
<h3><span class="header-section-number">5.1.3</span> R functions for silhouette analysis</h3>
<p>The silhouette coefficient of observations can be computed using the function <strong>silhouette()</strong> [in <strong>cluster</strong> package]:</p>
<pre class="r"><code>silhouette(x, dist, ...)</code></pre>
<ul>
<li><strong>x</strong>: an integer vector containing the cluster assignment of observations</li>
<li><strong>dist</strong>: a dissimilarity object created by the function <strong>dist()</strong></li>
</ul>
<p>The function <strong>silhouette()</strong> returns an object, of class <strong>silhouette</strong> containing:</p>
<ul>
<li>The <strong>cluster</strong> number of each observation i</li>
<li>The <strong>neighbor</strong> cluster of i (the cluster, not containing i, for which the average dissimilarity between its observations and i is minimal)</li>
<li>The <strong>silhouette width</strong> <span class="math">\(s_i\)</span> of each observation</li>
</ul>
<p>The R code below computes silhouette analysis and draw the result using R base plot:</p>
<pre class="r"><code># Silhouette coefficient of observations
library(&quot;cluster&quot;)
sil &lt;- silhouette(km.res$cluster, dist(iris.scaled))
head(sil[, 1:3], 10)</code></pre>
<pre><code>##       cluster neighbor sil_width
##  [1,]       1        3 0.7341949
##  [2,]       1        3 0.5682739
##  [3,]       1        3 0.6775472
##  [4,]       1        3 0.6205016
##  [5,]       1        3 0.7284741
##  [6,]       1        3 0.6098848
##  [7,]       1        3 0.6983835
##  [8,]       1        3 0.7308169
##  [9,]       1        3 0.4882100
## [10,]       1        3 0.6315409</code></pre>
<pre class="r"><code># Silhouette plot
plot(sil, main =&quot;Silhouette plot - K-means&quot;)</code></pre>
<p><img src="http://www.sthda.com/sthda/RDoc/figure/clustering/clustering-validation-statistics-k-means-clustering-silhouette-1.png" title="Clustering validation statistics - Unsupervised Machine Learning" alt="Clustering validation statistics - Unsupervised Machine Learning" width="518.4" /></p>
<p>Use <strong>factoextra</strong> for elegant data visualization:</p>
<pre class="r"><code>library(factoextra)
fviz_silhouette(sil)</code></pre>
<p>The summary of the <strong>silhouette analysis</strong> can be computed using the function <strong>summary.silhouette()</strong> as follow:</p>
<pre class="r"><code># Summary of silhouette analysis
si.sum &lt;- summary(sil)
# Average silhouette width of each cluster
si.sum$clus.avg.widths</code></pre>
<pre><code>##         1         2         3 
## 0.6363162 0.3473922 0.3933772</code></pre>
<pre class="r"><code># The total average (mean of all individual silhouette widths)
si.sum$avg.width</code></pre>
<pre><code>## [1] 0.4599482</code></pre>
<pre class="r"><code># The size of each clusters
si.sum$clus.sizes</code></pre>
<pre><code>## cl
##  1  2  3 
## 50 47 53</code></pre>
<p><span class="warning">Note that, if the clustering analysis is done using the function <strong>eclust()</strong>, cluster silhouettes are computed automatically and stored in the object <strong>silinfo</strong>. The results can be easily visualized as shown in the next sections.</span></p>
</div>
<div id="silhouette-plot-for-k-means-clustering" class="section level3">
<h3><span class="header-section-number">5.1.4</span> Silhouette plot for k-means clustering</h3>
<p>Its possible to draw <strong>silhouette plot</strong> using the function <strong>fviz_silhouette()</strong> [in <strong>factoextra</strong> package], which will also print a summary of the silhouette analysis output. To avoid this, you can use the option <strong>print.summary = FALSE</strong>.</p>
<pre class="r"><code># Default plot
fviz_silhouette(km.res)</code></pre>
<pre><code>##   cluster size ave.sil.width
## 1       1   50          0.64
## 2       2   47          0.35
## 3       3   53          0.39</code></pre>
<p><img src="http://www.sthda.com/sthda/RDoc/figure/clustering/clustering-validation-statistics-cluster-silhouette-plot-ggplot2-factoextra-2-1.png" title="Clustering validation statistics - Unsupervised Machine Learning" alt="Clustering validation statistics - Unsupervised Machine Learning" width="518.4" /></p>
<pre class="r"><code># Change the theme and color
fviz_silhouette(km.res, print.summary = FALSE) +
  scale_fill_brewer(palette = &quot;Dark2&quot;) +
  scale_color_brewer(palette = &quot;Dark2&quot;) +
  theme_minimal()+
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())</code></pre>
<p><img src="http://www.sthda.com/sthda/RDoc/figure/clustering/clustering-validation-statistics-cluster-silhouette-plot-ggplot2-factoextra-2-2.png" title="Clustering validation statistics - Unsupervised Machine Learning" alt="Clustering validation statistics - Unsupervised Machine Learning" width="518.4" /></p>
<p>Silhouette information can be extracted as follow:</p>
<pre class="r"><code># Silhouette information
silinfo &lt;- km.res$silinfo
names(silinfo)</code></pre>
<pre><code>## [1] &quot;widths&quot;          &quot;clus.avg.widths&quot; &quot;avg.width&quot;</code></pre>
<pre class="r"><code># Silhouette widths of each observation
head(silinfo$widths[, 1:3], 10)</code></pre>
<pre><code>##    cluster neighbor sil_width
## 1        1        3 0.7341949
## 41       1        3 0.7333345
## 8        1        3 0.7308169
## 18       1        3 0.7287522
## 5        1        3 0.7284741
## 40       1        3 0.7247047
## 38       1        3 0.7244191
## 12       1        3 0.7217939
## 28       1        3 0.7215103
## 29       1        3 0.7145192</code></pre>
<pre class="r"><code># Average silhouette width of each cluster
silinfo$clus.avg.widths</code></pre>
<pre><code>## [1] 0.6363162 0.3473922 0.3933772</code></pre>
<pre class="r"><code># The total average (mean of all individual silhouette widths)
silinfo$avg.width</code></pre>
<pre><code>## [1] 0.4599482</code></pre>
<pre class="r"><code># The size of each clusters
km.res$size</code></pre>
<pre><code>## [1] 50 47 53</code></pre>
</div>
<div id="silhouette-plot-for-pam-clustering" class="section level3">
<h3><span class="header-section-number">5.1.5</span> Silhouette plot for PAM clustering</h3>
<pre class="r"><code>fviz_silhouette(pam.res)</code></pre>
<pre><code>##   cluster size ave.sil.width
## 1       1   50          0.63
## 2       2   45          0.35
## 3       3   55          0.38</code></pre>
<p><img src="http://www.sthda.com/sthda/RDoc/figure/clustering/clustering-validation-statistics-pam-cluster-silhouette-1.png" title="Clustering validation statistics - Unsupervised Machine Learning" alt="Clustering validation statistics - Unsupervised Machine Learning" width="518.4" /></p>
</div>
<div id="silhouette-plot-for-hierarchical-clustering" class="section level3">
<h3><span class="header-section-number">5.1.6</span> Silhouette plot for hierarchical clustering</h3>
<pre class="r"><code>fviz_silhouette(res.hc)</code></pre>
<pre><code>##   cluster size ave.sil.width
## 1       1   49          0.75
## 2       2   75          0.37
## 3       3   26          0.51</code></pre>
<p><img src="http://www.sthda.com/sthda/RDoc/figure/clustering/clustering-validation-statistics-hierarchical-clustering-silhouette-1.png" title="Clustering validation statistics - Unsupervised Machine Learning" alt="Clustering validation statistics - Unsupervised Machine Learning" width="518.4" /></p>
</div>
<div id="samples-with-a-negative-silhouette-coefficient" class="section level3">
<h3><span class="header-section-number">5.1.7</span> Samples with a negative silhouette coefficient</h3>
<p>It can be seen that several samples have a negative silhouette coefficient in the hierarchical clustering. <strong>This means that they are not in the right cluster</strong>.</p>
<p>We can find the name of these samples and determine the clusters they are closer (neighbor cluster), as follow:</p>
<pre class="r"><code># Silhouette width of observation
sil &lt;- res.hc$silinfo$widths[, 1:3]
# Objects with negative silhouette
neg_sil_index &lt;- which(sil[, &#39;sil_width&#39;] &lt; 0)
sil[neg_sil_index, , drop = FALSE]</code></pre>
<pre><code>##     cluster neighbor   sil_width
## 51        2        3 -0.02848264
## 148       2        3 -0.03799687
## 129       2        3 -0.09622863
## 111       2        3 -0.14461589
## 109       2        3 -0.14991556
## 133       2        3 -0.18730218
## 42        2        1 -0.39515010</code></pre>
</div>
</div>
<div id="dunn-index" class="section level2">
<h2><span class="header-section-number">5.2</span> Dunn index</h2>
<div id="concept-and-algorithm-1" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Concept and algorithm</h3>
<p><strong>Dunn index</strong> is another internal clustering validation measure which can be computed as follow:</p>
<br/>
<div class="block">
<ol style="list-style-type: decimal">
<li>For each cluster, compute the distance between each of the objects in the cluster and the objects in the other clusters</li>
<li><p>Use the minimum of this pairwise distance as the inter-cluster separation (<strong>min.separation</strong>)</p></li>
<li>For each cluster, compute the distance between the objects in the same cluster.</li>
<li><p>Use the maximal intra-cluster distance (i.e maximum diameter) as the intra-cluster compactness</p></li>
<li><p>Calculate <strong>Dunn index</strong> (D) as follow:</p></li>
</ol>
<p><span class="math">\[
D = \frac{min.separation}{max.diameter}
\]</span></p>
</div>
<p><br/></p>
<p><span class="success">If the data set contains compact and well-separated clusters, the diameter of the clusters is expected to be small and the distance between the clusters is expected to be large. Thus, Dunn index should be maximized.</span></p>
</div>
<div id="r-function-for-computing-dunn-index" class="section level3">
<h3><span class="header-section-number">5.2.2</span> R function for computing Dunn index</h3>
<p>The function <strong>cluster.stats()</strong> [in <strong>fpc</strong> package] and the function <strong>NbClust()</strong> [in <strong>NbClust</strong> package] can be used to compute <strong>Dunn index</strong> and many other indices.</p>
<p>The function <strong>cluster.stats()</strong> is described in the next section.</p>
</div>
</div>
<div id="clustering-validation-statistics" class="section level2">
<h2><span class="header-section-number">5.3</span> Clustering validation statistics</h2>
<p>In this section, well describe the R function <strong>cluster.stats()</strong> [in <strong>fpc</strong> package] for computing a number of distance based statistics which can be used either for cluster validation, comparison between clustering and decision about the number of clusters.</p>
<p>The simplified format is:</p>
<pre class="r"><code>cluster.stats(d = NULL, clustering, al.clustering = NULL)</code></pre>
<br/>
<div class="block">
<ul>
<li><strong>d</strong>: a distance object between cases as generated by the <strong>dist()</strong> function</li>
<li><strong>clustering</strong>: vector containing the cluster number of each observation</li>
<li><strong>alt.clustering</strong>: vector such as for clustering, indicating an alternative clustering</li>
</ul>
</div>
<p><br/></p>
<p>The function <strong>cluster.stats()</strong> returns a list containing many components useful for analyzing the <strong>intrinsic characteristics</strong> of a clustering:</p>
<ul>
<li><strong>cluster.number</strong>: number of clusters</li>
<li><strong>cluster.size</strong>: vector containing the number of points in each cluster</li>
<li><strong>average.distance</strong>, <strong>median.distance</strong>: vector containing the cluster-wise within average/median distances</li>
<li><strong>average.between</strong>: average distance between clusters. We want it to be as large as possible</li>
<li><strong>average.within</strong>: average distance within clusters. We want it to be as small as possible</li>
<li><strong>clus.avg.silwidths</strong>: vector of cluster average silhouette widths. Recall that, the <strong>silhouette width</strong> is also an estimate of the average distance between clusters. Its value is comprised between 1 and -1 with a value of 1 indicating a very good cluster.</li>
<li><strong>within.cluster.ss</strong>: a generalization of the within clusters sum of squares (k-means objective function), which is obtained if d is a Euclidean distance matrix.</li>
<li><strong>dunn, dunn2</strong>: Dunn index</li>
<li><strong>corrected.rand, vi</strong>: Two indexes to assess the similarity of two clustering: the corrected Rand index and Meilas VI</li>
</ul>
<p>All the above elements can be used to evaluate the internal quality of clustering.</p>
<p>In the following sections, well compute the clustering quality statistics for k-means, pam and hierarchical clustering. Look at the <strong>within.cluster.ss</strong> (within clusters sum of squares), the <strong>average.within</strong> (average distance within clusters) and <strong>clus.avg.silwidths</strong> (vector of cluster average silhouette widths).</p>
<div id="cluster-statistics-for-k-means-clustering" class="section level4">
<h4><span class="header-section-number">5.3.0.1</span> Cluster statistics for k-means clustering</h4>
<pre class="r"><code>library(fpc)
# Compute pairwise-distance matrices
dd &lt;- dist(iris.scaled, method =&quot;euclidean&quot;)
# Statistics for k-means clustering
km_stats &lt;- cluster.stats(dd,  km.res$cluster)
# (k-means) within clusters sum of squares
km_stats$within.cluster.ss</code></pre>
<pre><code>## [1] 138.8884</code></pre>
<pre class="r"><code># (k-means) cluster average silhouette widths
km_stats$clus.avg.silwidths</code></pre>
<pre><code>##         1         2         3 
## 0.6363162 0.3473922 0.3933772</code></pre>
<pre class="r"><code># Display all statistics
km_stats</code></pre>
<pre><code>## $n
## [1] 150
## 
## $cluster.number
## [1] 3
## 
## $cluster.size
## [1] 50 47 53
## 
## $min.cluster.size
## [1] 47
## 
## $noisen
## [1] 0
## 
## $diameter
## [1] 5.034198 3.343671 2.922371
## 
## $average.distance
## [1] 1.175155 1.307716 1.197061
## 
## $median.distance
## [1] 0.9884177 1.2383531 1.1559887
## 
## $separation
## [1] 1.5533592 0.1333894 0.1333894
## 
## $average.toother
## [1] 3.647912 3.081212 2.674298
## 
## $separation.matrix
##          [,1]      [,2]      [,3]
## [1,] 0.000000 2.4150235 1.5533592
## [2,] 2.415024 0.0000000 0.1333894
## [3,] 1.553359 0.1333894 0.0000000
## 
## $ave.between.matrix
##          [,1]     [,2]     [,3]
## [1,] 0.000000 4.129179 3.221129
## [2,] 4.129179 0.000000 2.092563
## [3,] 3.221129 2.092563 0.000000
## 
## $average.between
## [1] 3.130708
## 
## $average.within
## [1] 1.222246
## 
## $n.between
## [1] 7491
## 
## $n.within
## [1] 3684
## 
## $max.diameter
## [1] 5.034198
## 
## $min.separation
## [1] 0.1333894
## 
## $within.cluster.ss
## [1] 138.8884
## 
## $clus.avg.silwidths
##         1         2         3 
## 0.6363162 0.3473922 0.3933772 
## 
## $avg.silwidth
## [1] 0.4599482
## 
## $g2
## NULL
## 
## $g3
## NULL
## 
## $pearsongamma
## [1] 0.679696
## 
## $dunn
## [1] 0.02649665
## 
## $dunn2
## [1] 1.600166
## 
## $entropy
## [1] 1.097412
## 
## $wb.ratio
## [1] 0.3904057
## 
## $ch
## [1] 241.9044
## 
## $cwidegap
## [1] 1.3892251 0.9432249 0.7824508
## 
## $widestgap
## [1] 1.389225
## 
## $sindex
## [1] 0.3524812
## 
## $corrected.rand
## NULL
## 
## $vi
## NULL</code></pre>
<p><span class="notice"> Read the documentation of <strong>cluster.stats()</strong> for details about all the available indices.</span></p>
<p>The same statistics can be computed for <strong>pam clustering</strong> and <strong>hierarchical clustering</strong>.</p>
</div>
<div id="cluster-statistics-for-pam-clustering" class="section level4">
<h4><span class="header-section-number">5.3.0.2</span> Cluster statistics for PAM clustering</h4>
<pre class="r"><code># Statistics for pam clustering
pam_stats &lt;- cluster.stats(dd,  pam.res$cluster)
# (pam) within clusters sum of squares
pam_stats$within.cluster.ss</code></pre>
<pre><code>## [1] 140.2856</code></pre>
<pre class="r"><code># (pam) cluster average silhouette widths
pam_stats$clus.avg.silwidths</code></pre>
<pre><code>##         1         2         3 
## 0.6346397 0.3496332 0.3823817</code></pre>
</div>
<div id="cluster-statistics-for-hierarchical-clustering" class="section level4">
<h4><span class="header-section-number">5.3.0.3</span> Cluster statistics for hierarchical clustering</h4>
<pre class="r"><code># Statistics for hierarchical clustering
hc_stats &lt;- cluster.stats(dd,  res.hc$cluster)
# (HCLUST) within clusters sum of squares
hc_stats$within.cluster.ss</code></pre>
<pre><code>## [1] 152.7107</code></pre>
<pre class="r"><code># (HCLUST) cluster average silhouette widths
hc_stats$clus.avg.silwidths</code></pre>
<pre><code>##         1         2         3 
## 0.6688130 0.3154184 0.4488197</code></pre>
</div>
</div>
</div>
<div id="external-clustering-validation" class="section level1">
<h1><span class="header-section-number">6</span> External clustering validation</h1>
<p>The aim is to compare the identified clusters (by k-means, pam or hierarchical clustering) to a reference.</p>
<p>To compare two cluster solutions, use the <strong>cluster.stats()</strong> function as follow:</p>
<pre class="r"><code>res.stat &lt;- cluster.stats(d, solution1$cluster, solution2$cluster)</code></pre>
<p>Among the values returned by the function <strong>cluster.stats()</strong>, there are two indexes to assess the similarity of two clustering, namely the corrected Rand index and Meilas VI.</p>
<p>We know that the <em>iris</em> data contains exactly 3 groups of species.</p>
<p><span class="question">Does the K-means clustering matches with the true structure of the data?</span></p>
<p>We can use the function <strong>cluster.stats()</strong> to answer to this question.</p>
<p>A cross-tabulation can be computed as follow:</p>
<pre class="r"><code>table(iris$Species, km.res$cluster)</code></pre>
<pre><code>##             
##               1  2  3
##   setosa     50  0  0
##   versicolor  0 11 39
##   virginica   0 36 14</code></pre>
<p>It can be seen that:</p>
<ul>
<li>All setosa species (n = 50) has been classified in cluster 1</li>
<li>A large number of versicor species (n = 39 ) has been classified in cluster 3. Some of them ( n = 11) have been classified in cluster 2.</li>
<li>A large number of virginica species (n = 36 ) has been classified in cluster 2. Some of them (n = 14) have been classified in cluster 3.</li>
</ul>
<p>Its possible to quantify the agreement between Species and k-means clusters using either the corrected Rand index and Meilas VI provided as follow:</p>
<pre class="r"><code>library(&quot;fpc&quot;)
# Compute cluster stats
species &lt;- as.numeric(iris$Species)
clust_stats &lt;- cluster.stats(d = dist(iris.scaled), 
                             species, km.res$cluster)
# Corrected Rand index
clust_stats$corrected.rand</code></pre>
<pre><code>## [1] 0.6201352</code></pre>
<pre class="r"><code># VI
clust_stats$vi</code></pre>
<pre><code>## [1] 0.7477749</code></pre>
<p><span class="success">The corrected <strong>Rand index</strong> provides a measure for assessing the similarity between two partitions, adjusted for chance. Its range is -1 (no agreement) to 1 (perfect agreement). Agreement between the specie types and the cluster solution is 0.62 using <strong>Rand index</strong> and 0.748 using Meilas VI</span></p>
<p>The same analysis can be computed for both <strong>pam</strong> and <strong>hierarchical clustering</strong>:</p>
<pre class="r"><code># Agreement between species and pam clusters
table(iris$Species, pam.res$cluster)</code></pre>
<pre><code>##             
##               1  2  3
##   setosa     50  0  0
##   versicolor  0  9 41
##   virginica   0 36 14</code></pre>
<pre class="r"><code>cluster.stats(d = dist(iris.scaled), 
              species, pam.res$cluster)$vi</code></pre>
<pre><code>## [1] 0.7129034</code></pre>
<pre class="r"><code># Agreement between species and HC clusters
table(iris$Species, res.hc$cluster)</code></pre>
<pre><code>##             
##               1  2  3
##   setosa     49  1  0
##   versicolor  0 50  0
##   virginica   0 24 26</code></pre>
<pre class="r"><code>cluster.stats(d = dist(iris.scaled), 
              species, res.hc$cluster)$vi</code></pre>
<pre><code>## [1] 0.6097098</code></pre>
<p><span class="success">External clustering validation, can be used to select suitable clustering algorithm for a given dataset.</span></p>
</div>
<div id="infos" class="section level1">
<h1><span class="header-section-number">7</span> Infos</h1>
<p><span class="warning">This analysis has been performed using <strong>R software</strong> (ver. 3.2.1)</span></p>
<ul>
<li>Malika Charrad, Nadia Ghazzali, Veronique Boiteau, Azam Niknafs (2014). NbClust: An R Package for Determining the Relevant Number of Clusters in a Data Set. Journal of Statistical Software, 61(6), 1-36. URL <a href="http://www.jstatsoft.org/v61/i06/" class="uri">http://www.jstatsoft.org/v61/i06/</a>.</li>
<li>Theodoridis S, Koutroubas K (2008). Pattern Recognition. 4th edition. Academic Press.</li>
</ul>
</div>

<script>jQuery(document).ready(function () {
	jQuery('h1').addClass('wiki_paragraph1');
	jQuery('h2').addClass('wiki_paragraph2');
	jQuery('h3').addClass('wiki_paragraph3');
	jQuery('h4').addClass('wiki_paragraph4');
	});//add phpboost class to header</script>
<style>.content{padding:0px;}</style>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</div><!--end rdoc-->
<!--====================== stop here when you copy to sthda================-->

<!-- END HTML -->
                        </div>
                        <br/>
                        <br/>
                        <!-- laddthis, ike -->
                        <div class="addthis_native_toolbox"></div>

                        <br/>
                        <br/> 
                        <div>
							<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
                            <!-- lien_200X90 -->
                            <ins class="adsbygoogle"
                                 style="display:inline-block;width:200px;height:90px"
                                 data-ad-client="ca-pub-5474463749888038"
                                 data-ad-slot="7994647366"></ins>
                            <script>
                            (adsbygoogle = window.adsbygoogle || []).push({});
                            </script>
                        </div>
                        <br/><br/>
                        
                        <center>


                            <br/><br/><br/>
                             <div>
                                <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
                                <!-- 336X280_text_only -->
                                <ins class="adsbygoogle"
                                     style="display:inline-block;width:336px;height:280px"
                                     data-ad-client="ca-pub-5474463749888038"
                                     data-ad-slot="4090131761"></ins>
                                <script>
                                (adsbygoogle = window.adsbygoogle || []).push({});
                                </script>
                            </div>

                        </center>


                     </div>
                     <!-- end of content -->
                    
                   
                    <div style="clear:both;"></div>
               </div> <!--end of sticky-parent-->
                
                
                <!-- ===========Add by AKASSAMBARA ============
                 -->
                 
             <div>
             
             
             	<br/>
                
                
                <!-- get involved -->
                <div class="block get_involved">
                	<strong><i class="fa fa-2x fa-group"></i>&nbsp;Get involved : </strong><br/>
            	 	<i class="fa fa-share fa-2x"></i>&nbsp;
                    	Click to <b>follow us</b> on <a href="https://www.facebook.com/1570814953153056" class="facebook" target="_blank">Facebook</a> and 
                         <a href="https://plus.google.com/108962828449690000520" rel="publisher">Google+</a> : 
                         <a href="https://www.facebook.com/1570814953153056" class="facebook" target="_blank"><i class="fa fa-facebook-square fa-2x"></i></a>&nbsp;&nbsp;
                        <a href="https://plus.google.com/108962828449690000520" rel="publisher" class="google" target="_blank"><i class="fa fa-google-plus-square fa-2x"></i></a><br/>
                        
                     <i class="fa fa-comment fa-2x"></i>&nbsp; <b>Comment this article</b> by clicking on "Discussion" button (top-right position of this page)<br/>
                     <i class="fa fa-user fa-2x"></i>&nbsp; <a href="../user/registration/">Sign up as a member</a> and post <a href="how-to-contribute-to-sthda-web-site">news and articles</a> on STHDA web site.<br/>
            	 </div>
               </div>
                 
                            
                <!--=============== Related articles================ -->
                <br/><br/>
                 
                    <!--articles dans la mÃªme categorie -->
                    <div class="related_article">
                        <h1 class="wiki_paragraph1">Suggestions</h1> <br/>
                          
                        <div>
                             <i class="fa fa-file"></i> <a href="model-based-clustering-unsupervised-machine-learning">Model-Based Clustering - Unsupervised Machine Learning</a><br /> <i class="fa fa-file"></i> <a href="determining-the-optimal-number-of-clusters-3-must-known-methods-unsupervised-machine-learning">Determining the optimal number of clusters: 3 must known methods - Unsupervised Machine Learning</a><br /> <i class="fa fa-file"></i> <a href="hierarchical-clustering-essentials-unsupervised-machine-learning">Hierarchical Clustering Essentials - Unsupervised Machine Learning</a><br /> <i class="fa fa-file"></i> <a href="partitioning-cluster-analysis-quick-start-guide-unsupervised-machine-learning">Partitioning cluster analysis: Quick start guide - Unsupervised Machine Learning</a><br /> <i class="fa fa-file"></i> <a href="beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning">Beautiful dendrogram visualizations in R: 5+ must known methods - Unsupervised Machine Learning</a><br /> <i class="fa fa-file"></i> <a href="dbscan-density-based-clustering-for-discovering-clusters-in-large-datasets-with-noise-unsupervised-machine-learning">DBSCAN: density-based clustering for discovering clusters in large datasets with noise - Unsupervised Machine Learning</a><br /> <i class="fa fa-file"></i> <a href="clarifying-distance-measures-unsupervised-machine-learning">Clarifying distance measures - Unsupervised Machine Learning</a><br /> <i class="fa fa-file"></i> <a href="assessing-clustering-tendency-a-vital-issue-unsupervised-machine-learning">Assessing clustering tendency: A vital issue - Unsupervised Machine Learning</a><br /> <i class="fa fa-file"></i> <a href="the-guide-for-clustering-analysis-on-a-real-data-4-steps-you-should-know-unsupervised-machine-learning">The Guide for Clustering Analysis on a Real Data: 4 steps you should know - Unsupervised Machine Learning</a><br /> <i class="fa fa-file"></i> <a href="how-to-choose-the-appropriate-clustering-algorithms-for-your-data-unsupervised-machine-learning">How to choose the appropriate clustering algorithms for your data? - Unsupervised Machine Learning</a><br /> <i class="fa fa-file"></i> <a href="hcpc-hierarchical-clustering-on-principal-components-hybrid-approach-2-2-unsupervised-machine-learning">HCPC: Hierarchical clustering on principal components - Hybrid approach (2/2) - Unsupervised Machine Learning</a><br /> <i class="fa fa-file"></i> <a href="visual-enhancement-of-clustering-analysis-unsupervised-machine-learning">Visual Enhancement of Clustering Analysis - Unsupervised Machine Learning</a><br /> <i class="fa fa-file"></i> <a href="how-to-compute-p-value-for-hierarchical-clustering-in-r-unsupervised-machine-learning">How to compute p-value for hierarchical clustering in R - Unsupervised Machine Learning</a><br /> <i class="fa fa-file"></i> <a href="clustering-unsupervised-machine-learning">Clustering - Unsupervised machine learning</a><br /> <i class="fa fa-file"></i> <a href="hybrid-hierarchical-k-means-clustering-for-optimizing-clustering-outputs-unsupervised-machine-learning">Hybrid hierarchical k-means clustering for optimizing clustering outputs - Unsupervised Machine Learning</a><br />
                         </div>
                    </div>
                    <br/>
                    <div>
                       <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
                        <!-- 728X90 -->
                        <ins class="adsbygoogle"
                             style="display:inline-block;width:728px;height:90px"
                             data-ad-client="ca-pub-5474463749888038"
                             data-ad-slot="6756867106"></ins>
                        <script>
                        (adsbygoogle = window.adsbygoogle || []).push({});
                        </script>
                    </div>
                
                
                 
             <!-- ======================END of related articles =================-->
             
            
            
             
             
          </div>
                      
                
				
				<div class="spacer" style="margin-top:30px;">&nbsp;</div>
			</div>
			<footer>
				<div style="text-align:center;margin-top:8px;margin-bottom:10px;">This page has been seen 2532 times</div>
			</footer>
		</article>
        
        
  
  
  <script type="text/javascript">
  jQuery(document).ready(function(){
	 jQuery("#aksidebar, #ak_main").stick_in_parent({parent: "#sticky-parent", spacer: ".sticky-content-spacer"});

	//involv visitors
	 setGetInvolvedBlock(getLang());
	}); 
</script> 

</div>

			</div>
			
		</div>
		
		<div id="top-footer">
			
<div id="newsletter">
	<form action="/english/newsletter/?url=/subscribe/" method="post">
		<div class="newsletter-form input-element-button">
			<span class="newsletter-title">Newsletter</span> 
			<input type="text" name="mail_newsletter" maxlength="50" value="" placeholder="Email">
			<input type="hidden" name="subscribe" value="subscribe">
			<input type="hidden" name="token" value="74c136fb55c47d73">
			<button type="submit" class="newsletter-submit"><i class="fa fa-envelope-o"></i></button>
		</div>
	</form>
</div>

			<div class="spacer"></div>
		</div>
		
		
		<div class="spacer"></div>
	</div>
    
	<footer id="footer">
		
		<div class="footer-infos">
        
        	<div id="footer_columns_container">
		
        		<!--
                <div class="footer_columns">
                    <div class="footer_columns_title"> 
                        <img src="/english/templates/sthda/theme/images/icones/connexion-et-inscription.png" align="middle">
                        Inscrivez-vous
                    </div>
                    <ul>
                        <li><a href="/user/connect">Connectez-vous</a></li>
                        <li><a href="/user/registration">CrÃ©er un compte</a></li>
                        <li><a href="/user/password/lost">Mot de passe oubliÃ© ?</a></li>
    
                    </ul>
                </div>	
                
                 <div class="footer_columns">
                    <div class="footer_columns_title"> 
                        <img src="/english/templates/sthda/theme/images/icones/connexion-et-inscription.png" align="middle">
                        Inscrivez-vous
                    </div>
                    <ul>
                        <li><a href="/user/connect">Connectez-vous</a></li>
                        <li><a href="/user/registration">CrÃ©er un compte</a></li>
                        <li><a href="/user/password/lost">Mot de passe oubliÃ© ?</a></li>
    
                    </ul>
                </div>	
                
                 <div class="footer_columns">
                    <div class="footer_columns_title"> 
                        <img src="/english/templates/sthda/theme/images/icones/connexion-et-inscription.png" align="middle">
                        Inscrivez-vous
                    </div>
                    <ul>
                        <li><a href="/user/connect">Connectez-vous</a></li>
                        <li><a href="/user/registration">CrÃ©er un compte</a></li>
                        <li><a href="/user/password/lost">Mot de passe oubliÃ© ?</a></li>
    
                    </ul>
                </div>	
                
            </div>
            
            <div style="clear:both;"></div
            -->
            
            <span>
            	<a href="/english/sitemap/">Sitemap</a> |
        	</span>
			<span>
				Boosted by <a href="http://www.phpboost.com" title="PHPBoost">PHPBoost 4.0</a> 
			</span>	
			
			
		</div>
	</footer>
    
     <!-- JQwidgets
    =============================== -->  
    <link rel="stylesheet" href="/english/rsthda/templates/scripts/jqwidgets-ver3.5.0//styles/jqx.base.css" type="text/css" />
    <link rel="stylesheet" href="/english/rsthda/templates/scripts/jqwidgets-ver3.5.0//styles/jqx.ui-start.css" type="text/css" />
    <script type="text/javascript" src="/english/rsthda/templates/scripts/jqwidgets-ver3.5.0/jqxcore.js"></script>
    <script type="text/javascript" src="/english/rsthda/templates/scripts/jqwidgets-ver3.5.0/jqxmenu.js"></script>
    <script type="text/javascript" src="/english/rsthda/templates/scripts/jqwidgets-ver3.5.0/jqxbuttons.js"></script>

    <!--- ak -->
    <script type="text/javascript" src="/english/templates/sthda/ak/global.js"></script>
    <script type="text/javascript" src="/english/sthda/js/jquery.sticky-kit.min.js"></script><!--fixation d'un div -->
    
       <!-- GOOgle doc viewer : permet de visualiser des documents embarquÃ©s online
    http://www.jawish.org/blog/archives/394-Google-Docs-Viewer-plugin-for-jQuery.html
    https://docs.google.com/viewer
    -->
    <script type="text/javascript" src="/english/sthda/js/jquery.gdocsviewer.min.js"></script>
    <script type="text/javascript"> 
    /*<![CDATA[*/
    
    jQuery(document).ready(function() {
       if(jQuery('a.embed').length!=0) jQuery('a.embed').gdocsViewer({width: "98%", height: 600});
        if(jQuery('a.view').length!=0) jQuery('a.view').gdocsViewer({width: "98%", height: 600});
    });
    /*]]>*/
    </script>  
    
     <!-- R knitr -->
    <link rel="stylesheet" href="/english/sthda/RDoc/libs/style.css"/>
    <script src="/english/sthda/RDoc/libs/highlight.js"></script>
    <script type="text/javascript">
    if (window.hljs && document.readyState && document.readyState === "complete") {
       window.setTimeout(function() {
          hljs.initHighlighting();
       }, 0);
    }
    </script>
   
     <!--=================================
     Generer automatiquement une table des matiÃ¨re (TOC)
     #Utilisation : placer <ul id="toc"></ul> ou <ol id="toc"></ol> Ã  l'endroit de votre page oÃ¹ vous souhaitez mettre la table des matiÃ¨res
     #Lien : http://fuelyourcoding.com/scripts/toc/index.html
    ================================= -->
    <script type="text/javascript" src="/english/sthda/js/jquery.tableofcontents.min.js"></script>
    <script type="text/javascript"> 
    /*<![CDATA[*/
        jQuery(document).ready(function(){ 
        if(jQuery('ul#toc').length!=0) {
            jQuery("ul#toc").tableOfContents(
                null,                        // Default scoping
                    {
                      startLevel:           2,   // H2
                      depth:                3,   // H1 through 3
                      
                    }
            ); 
        }
        });
    /*]]>*/
    </script>

     <style>
    ul#toc{
        float: right;font-size: 10pt;
        width: 270px;padding: 10px 10px 10px 20px;border: solid 1px #ccd136;margin: 0 0 10px 15px;border: 1px solid #CCCCCC;
        border-radius: 5px;box-shadow: 2px 2px 10px -2px #666666;background-color: #f6f6f6;
    }
    /*fait un retrait Ã  chaque niveau hiÃ©rarchique*/	
    #toc ul,  #toc ol{padding-left:30px;}
    </style>
 <!--================END TOC================= --> 

 
 
 
 <!-- Go to www.addthis.com/dashboard to customize your tools 
 right side-->
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-50f0e98f7770f530"></script>
<!-- Recommended for you->
<!-- Go to www.addthis.com/dashboard to customize your tools -->
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-50f0e98f7770f530" async></script>
<!-- Go to www.addthis.com/dashboard to customize your tools -->
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-50f0e98f7770f530" async></script>



				<script src="/english/kernel/lib/js/bottom.js"></script>
		<!--[if lt IE 9]>
		<script async src="/english/kernel/lib/js/html5shiv/html5shiv.js"></script>
		<![endif]-->
		<script>
		<!-- 
			$$('[data-confirmation]').each(function(a) {
				var data_confirmation = a.readAttribute('data-confirmation');
				
				if (data_confirmation == 'delete-element')
					var message = 'Do you really want to delete this item ?';
				else
					var message = data_confirmation;

				a.onclick = function () { return confirm(message); }
			}); 
		-->
		</script>
	</body>
</html>